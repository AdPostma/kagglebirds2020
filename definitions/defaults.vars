# default settings
# You'd usually leave these unchanged, and override specific settings
# via another file passed with --vars, or directly via --var KEY=VALUE.

# debugging mode: more prints, anomaly detection, exception on NaN
debug=0

# dataset settings
# see further settings in definitions/datasets/kagglebirds2020/defaults.vars
dataset=kagglebirds2020
# examples per minibatch
batchsize=16
# whether and how many background processes to use for data loading
data.num_workers=0
# whether to use pinned memory for data loading (only useful with CUDA)
data.pin_memory=0

# model settings
# see further settings in definitions/models/audioclass/defaults.vars
model=audioclass
# seed used for model initialization (random if omitted)
model.init_seed=
# initialize weights from existing model file, relative to the repository
model.init_from=
# custom initialization: kaiming/xavier:<leakiness>, const:<val>, icnr:<init>
model.init.conv_weight=
model.init.conv_bias=
model.init.conv_transposed_weight=
model.init_conv_strided_weight=

# metrics settings
# see definitions/metrics/defaults.vars

# training settings
# also see definitions/optimizers/defaults.vars
# minibatches per mini-epoch
train.epochsize=1000
# maximum number of mini-epochs to train for
train.epochs=1000
# initial learning rate
train.eta=1e-3
# reference value to monitor for learning rate adjustments and early stopping
# (prepend a minus sign to negate the value)
train.patience_reference=valid_loss
# after no improvement for this many mini-epochs, reduce learning rate
train.patience=10
# factor by which to multiply learning rate whenever patience runs out
train.eta_decay=0.1
# after each drop, wait for these many mini-epochs before getting impatient
train.cooldown=0
# minimum learning rate; learning stops when eta falls below this value
train.min_eta=1e-5
# optionally treat a given number of parameters at the beginning of the model
# differently (either specify the number, the name of a submodel, or multiple
# submodels separated by plus signs)
train.first_params=0
# start training these parameters only from the given epoch
train.first_params.delay=0
# use an extra scaling factor for those parameter's eta
train.first_params.eta_scale=1
# whether to do mixed-precision training
float16=0
float16.opt_level=O2

# progressbar settings
# - only use ASCII chars: set to 1 if terminal is falsely detected as unicode
tqdm.ascii=0
